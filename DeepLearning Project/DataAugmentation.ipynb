{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "machine_shape": "hm",
      "gpuType": "L4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from albumentations import (\n",
        "    HorizontalFlip, VerticalFlip, RandomRotate90, RandomBrightnessContrast,\n",
        "    ElasticTransform, GridDistortion, ShiftScaleRotate\n",
        ")\n",
        "from albumentations.core.composition import Compose\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "\n",
        "def augment_dataset(image_dir, mask_dir, save_dir, max_augmentations=2646):\n",
        "    \"\"\"Augmenter le dataset jusqu'à un maximum d'images spécifié.\"\"\"\n",
        "    # Liste des transformations\n",
        "    transforms = Compose([\n",
        "        HorizontalFlip(p=0.5),\n",
        "        VerticalFlip(p=0.5),\n",
        "        RandomRotate90(p=0.5),\n",
        "        RandomBrightnessContrast(p=0.5),\n",
        "        ElasticTransform(p=0.5, alpha=120, sigma=120 * 0.05, approximate=True),\n",
        "        GridDistortion(p=0.5),\n",
        "        ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5)\n",
        "    ])\n",
        "\n",
        "    # Charger les chemins d’images et de masques\n",
        "    image_paths = sorted([os.path.join(image_dir, img) for img in os.listdir(image_dir)])\n",
        "    mask_paths = sorted([os.path.join(mask_dir, mask) for mask in os.listdir(mask_dir)])\n",
        "\n",
        "    # Créer des dossiers pour sauvegarder les augmentations\n",
        "    os.makedirs(os.path.join(save_dir, \"images\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(save_dir, \"masks\"), exist_ok=True)\n",
        "\n",
        "    # Compteur total\n",
        "    total_augmentations = 0\n",
        "\n",
        "    # Boucler sur chaque image et son masque\n",
        "    for img_path, mask_path in tqdm(zip(image_paths, mask_paths), total=len(image_paths)):\n",
        "        image = cv2.imread(img_path)\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # Ajouter des augmentations tant que le total n’est pas atteint\n",
        "        while total_augmentations < max_augmentations:\n",
        "            augmented = transforms(image=image, mask=mask)\n",
        "            aug_image = augmented[\"image\"]\n",
        "            aug_mask = augmented[\"mask\"]\n",
        "\n",
        "            # Sauvegarder les augmentations\n",
        "            img_name = f\"aug_{total_augmentations}.png\"\n",
        "            mask_name = f\"aug_{total_augmentations}.png\"\n",
        "            cv2.imwrite(os.path.join(save_dir, \"images\", img_name), aug_image)\n",
        "            cv2.imwrite(os.path.join(save_dir, \"masks\", mask_name), aug_mask)\n",
        "\n",
        "            total_augmentations += 1\n",
        "\n",
        "            # Arrêter si on atteint le maximum\n",
        "            if total_augmentations >= max_augmentations:\n",
        "                break\n",
        "\n",
        "    print(f\"Augmentation terminée. Total des images : {total_augmentations}\")\n",
        "\n",
        "\n",
        "def split_train_valid(augmented_data_path, train_output_path, valid_output_path, test_size=0.2):\n",
        "    \"\"\"Diviser le dataset augmenté en Train et Valid.\"\"\"\n",
        "    # Créer les dossiers Train et Valid\n",
        "    os.makedirs(os.path.join(train_output_path, \"images\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(train_output_path, \"masks\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(valid_output_path, \"images\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(valid_output_path, \"masks\"), exist_ok=True)\n",
        "\n",
        "    # Charger toutes les images et masques\n",
        "    images = sorted(glob(os.path.join(augmented_data_path, \"images\", \"*.png\")))\n",
        "    masks = sorted(glob(os.path.join(augmented_data_path, \"masks\", \"*.png\")))\n",
        "\n",
        "    # Vérification\n",
        "    assert len(images) == len(masks), \"Le nombre d'images et de masques doit être identique\"\n",
        "\n",
        "    # Diviser les données en Train et Valid\n",
        "    train_images, valid_images, train_masks, valid_masks = train_test_split(\n",
        "        images, masks, test_size=test_size, random_state=42\n",
        "    )\n",
        "\n",
        "    # Copier les fichiers dans les dossiers correspondants\n",
        "    def copy_files(file_list, dest_folder):\n",
        "        for file_path in file_list:\n",
        "            file_name = os.path.basename(file_path)\n",
        "            shutil.copy(file_path, os.path.join(dest_folder, file_name))\n",
        "\n",
        "    copy_files(train_images, os.path.join(train_output_path, \"images\"))\n",
        "    copy_files(train_masks, os.path.join(train_output_path, \"masks\"))\n",
        "    copy_files(valid_images, os.path.join(valid_output_path, \"images\"))\n",
        "    copy_files(valid_masks, os.path.join(valid_output_path, \"masks\"))\n",
        "\n",
        "    print(f\"Nombre d'images d'entraînement : {len(train_images)}\")\n",
        "    print(f\"Nombre d'images de validation : {len(valid_images)}\")\n",
        "\n",
        "\n",
        "# Définir les chemins\n",
        "image_dir = \"/content/drive/MyDrive/DATASET/DATASETNOUR/Training/images\"\n",
        "mask_dir = \"/content/drive/MyDrive/DATASET/DATASETNOUR/Training/masks\"\n",
        "save_dir = \"/content/drive/MyDrive/DATASET/Augmented_Dataset\"\n",
        "train_output_path = os.path.join(save_dir, \"train\")\n",
        "valid_output_path = os.path.join(save_dir, \"valid\")\n",
        "\n",
        "# Étape 1 : Augmenter le dataset\n",
        "augment_dataset(image_dir, mask_dir, save_dir, max_augmentations=2646)\n",
        "\n",
        "# Étape 2 : Diviser en Train et Valid\n",
        "split_train_valid(save_dir, train_output_path, valid_output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "P0GRW6rn5KZD",
        "outputId": "2754bad3-1e36-4661-e793-11a2a6658b12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/42 [31:20<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-f97455de1062>\u001b[0m in \u001b[0;36m<cell line: 106>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;31m# Étape 1 : Augmenter le dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m \u001b[0maugment_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_augmentations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2646\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;31m# Étape 2 : Diviser en Train et Valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-f97455de1062>\u001b[0m in \u001b[0;36maugment_dataset\u001b[0;34m(image_dir, mask_dir, save_dir, max_augmentations)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mmask_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"aug_{total_augmentations}.png\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"images\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maug_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"masks\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maug_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mtotal_augmentations\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}